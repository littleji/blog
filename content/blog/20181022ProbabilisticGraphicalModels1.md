+++
title = "概率图模型之一(ProbabilisticGraphicalModel-PGM)"
date = 2018-10-22 17:50:20
updated = 2025-02-08 17:00:00
description = ""

[taxonomies]
tags = ["概率图(PGM)", "机器学习(MachineLearning)"]

[extra]
quick_navigation_buttons = true
toc = true
mermaid = true
+++
# 概率图概述
概率图模型使用图的方法来表示概率分布，在该模型中，结点表示变量，节点之间的边表示变量之间的概略关系。

根据图模型中的边是否有向，将概率图的代表模型如下：
￼
![image](https://user-images.githubusercontent.com/7655877/47288361-5f420c00-d628-11e8-9ad1-555e47176c16.png)

## 无向图模型和有向图模型的区别
这里要说明的是,采用有向图的贝叶斯网络的"有向"表示的是依存关系,具有因果推断,即A->B->C,反之则不可.
而无向图的代表马尔科夫网络,采用的无向图


各个图模型的演变关系如下：

![image](https://user-images.githubusercontent.com/7655877/47288381-7a148080-d628-11e8-836a-fbe66c555d01.png)

其中横向，由点到线（序列结构），最终到面。
纵向则是在一定的条件下生成式模型转换为判别式模型。
下面假设有观测序列x，状态序列y，并依次来说明生成式与判别式模型的区别。

## 生成式模型
生成式模型的定义是：“状态（输出）序列y按照一定的规律生成观测（输入）序列x”。生成式模型的本质是对于联合概率分布p(x,y)进行建模，并根据生成概率最大的生成序列来获取y。

这类模型中，一般有严格的独立性假设，模型变量之间的关系清楚，处理单类问题时较为灵活。

弱点是模型的推导与学习较为复杂。

主要的模型有：n-gram，HMM、朴素贝叶斯、概率上下文无关文法。

## 判别式模型
判别式模型的定义是：“状态（输出）序列y是由观测序列（输入）所决定的。”判别式模型的本质是对后验概率p（y|x）进行建模，优点是处理多累问题或分辨某一类与其他类差异时更为灵活，模型构造简单。

缺点是模型的描述能力有限，变量之间的关系不清楚。大多数模型都是有监督学习，不能很好地扩展为无监督学习。

主要模型有：最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场、感知机。

# 贝叶斯网络
贝叶斯网络也称为belief networks，是一种基于概率推理的数学模型，理论基础为贝叶斯公式。

贝叶斯网络形式上是一个有向无环图（DAG directed acyclic graph），结点表示随机变量，结点之间的边表示条件依存关系，箭头出发的节点为父节点，箭头到达的节点为子节点，子节点依存于父节点。

如果两个节点没有连接关系，表示两个随机变量能够在某些特定情况下条件独立。

## 构造贝叶斯网络
构造贝叶斯网络是一个复杂的任务，其主要有三个方面的问题：表示、推断、学习

### 表示
在简单某一随机变量的组合上
![image](https://user-images.githubusercontent.com/7655877/47356431-a4cd0a80-d6f6-11e8-9304-e72390800815.png)
即便是随机变量只有两个取值，那么联合概率分布P需要对 2^n 种不同取值下的概率情况进行说明，然而这件事的计算代价非常高。

### 推断
由于贝叶斯网络是变量以及关系的完整模型，那么在观测到某些变量变化的时候就需要使用一些推断方法，来得知另一些变量子集的变化。
概率推理：在已知某些证据的情况下，计算变量的后验分布的过程。
常用的精确推理方法有两种
 * 变量消除法 variable elimination
 * 团树法 clique tree

常用的近似推理
 * 重要性抽样 importance sampling
 * 随机马尔科夫链蒙特卡洛模拟法 Markov chain Monte Carlo，MCMC
 * 循环信念传播法 loopy belief propagation
 * 泛化信念传播法 generalized belief propagation

### 学习
参数学习的目的就是得知各个变量结点之间相互依存的强度。
比如给定某个节点X，需要计算 P(X|父节点1，父节点2...)，这些概率分布可以是任意形式，通常是离散分布与高斯分布。

常用的参数学习方法包括：
 * 最大似然估计 MLE
 * 最大后验概率 MAP
 * 期望最大 EM
 * 贝叶斯估计方法，贝叶斯图模型中，使用较多的是该种方法

除了参数学习，还需要结构学习来学习各个变量之间的图关系，简单的贝叶斯可以由有经验的专家来构造，但一般情况下人工构造一个贝叶斯网络的结构几乎不可能，所以自动结构学习是一项颇具挑战的任务。


## MLE MAP 贝叶斯估计之间的关系

其中，最大似然估计（MLE）是频率派的代表，贝叶斯估计（Bayes）是贝叶斯派的代表，最大后验估计是频率派和贝叶斯派的合成，是一种规则化后的最大似然估计。

一般来说，在我们对于先验概率一无所知时，只能假设每种猜测的先验概率是均等的（其实这也是人类经验的结果），这个时候就只有用最大似然了，但不能忘记一个重要的前提是所有的采样都是独立同分布。

如果我们有足够的自信，训练集中的样本分布的确很接近真实的情况，这时就应该用贝叶斯方法。贝叶斯学派强调的是“靠谱的先验概率”。所以说贝叶斯学派的适用范围更广，关键要先验概率靠谱，而频率学派有效的前提也是他们的先验概率同样是经验统计的结果。但也说明了贝叶斯计算要更为复杂，因为需要选择一个常用分布，并确定一个初始参数集作为先验分布。

再者，MAP与MLE最大区别是MAP中加入了模型参数本身的概率分布，即是否考虑了先验知识。或者说。MLE中认为模型参数本身的概率的是均匀的，即该概率为一个固定值。






# 参考
统计自然语言处理（第2版）
[1](https://blog.csdn.net/Mr_tyting/article/details/62882162?utm_source=blogxgwz1)
[2](https://blog.csdn.net/juanjuan1314/article/details/78189527)
[3](https://zhuanlan.zhihu.com/p/32568242)
[4](https://zhuanlan.zhihu.com/p/32616870)
[5](https://blog.csdn.net/guohecang/article/details/52313046)
